---
title: "The tidyverse: dplyr, ggplot2, and friends"
output: html_document
---

This lesson covers packages primarily by Hadley Wickham for tidying data and then working with it in tidy form.

The packages we are using in this lesson are all from CRAN, so we can install them with `install.packages`.

```{r eval=FALSE}
install.packages(c(
    # Hadley Wickham packages
    "readr",    # read tabular data
    "tidyr",    # data frame tidying functions
    "dplyr",    # general data frame manipulation
    "ggplot2",  # flexible plotting
    
    # Viridis color scale 
    "viridis",

    # (Advanced!) Tidy linear modelling    
    "broom"
))
```

```{r warning=FALSE, message=FALSE}
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(viridis)
library(broom)
```

These packages usually have useful documentation in the form of "vignettes". These are readable on the CRAN website, or within R:

```{r eval=FALSE}
vignette()
vignette(package="dplyr")
vignette("introduction", package="dplyr")
```


# dplyr

Let's continue our examination of the FastQC output. If you're starting fresh for this lesson, you can load the necessary data frame with:

```{r}
bigtab <- read_csv("r-more-files/fastqc.csv")
bigtab$grade <- factor(bigtab$grade, c("FAIL","WARN","PASS"))

bigtab
```

`bigtab` is a "tibble", which is Hadley Wickham's slightly improved data frame. One convenient feature is that it only shows a few rows of the data frame when you print it. If you do want to see the whole table, you can use `as.data.frame`, or use the `View` function to view it in a tab.

```{r eval=FALSE}
as.data.frame(bigtab)
View(bigtab)
```

`dplyr` gives us a collection of convenient "verbs" for manipulating data frames.

## filter

Say we want to know all the tests that failed. `filter` provides a convenient way to get these.

```{r}
filter(bigtab, grade == "FAIL")
```

Something is magic here: we do not have `grade` in our environment, only within the data frame.

## arrange

Rather than filtering, we might instead want to sort the data so the most important rows are at the top. `arrange` sorts a data frame by one or more columns.

```{r}
arrange(bigtab, grade)

# desc( ) can be used to reverse the sort order
arrange(bigtab, desc(grade))
```


## select

dplyr's `select` function is for subsetting, renaming, and reordering *columns*. Old columns can be referred to by name or by number.

```{r}
select(bigtab, test,grade) 
select(bigtab, 2,1)
select(bigtab, foo=file, bar=test, baz=grade)
```

You can also remove specific columns like this:

```{r}
select(bigtab, -file)
```


## Joins

Say we want to convert PASS/WARN/FAIL into a numeric score, so we can produce some numerical summaries. The scoring scheme will be:

```{r}
scoring <- tibble(
    grade=c("FAIL","WARN","PASS"),
    score=c(0,0.5,1))

# tibble is the Hadley Wickham data frame.
# It's better in various ways, eg it prints itself nicely.

# Could instead use: 
# scoring <- data.frame(
#    grade=c("FAIL","WARN","PASS"),
#    score=c(0,0.5,1))

scoring
```

In the introduction to R day, we saw the `merge` function. dplyr has its own version of this in the form of several functions: `left_join`, `right_join`, `inner_join`, `outer_join`.

The difference between these functions is what happens when there is a row in one data frame without a corresponding row in the other data frame. `inner_join` discards such rows. `outer_join` always keeps them, filling in missing data with NA. `left_join` always keeps rows from the first data frame. `right_join` always keeps rows from the second data frame.

Often we are use a join to augment a data frame with some extra information. `left_join` is a good default choice for this as it will never delete rows from the data frame that is being augmented.

```{r}
scoretab <- left_join(bigtab, scoring, by="grade")
scoretab
```

"grade" is here acting as the *key*, allowing corresponding rows in the data frames to be joined together.

One important thing that all the join functions do: if multiple rows have the same key in either data frame, all ways of combining the two sets of rows will be included in the result. So, here, rows from the scoring data frame have been copied many times in the output. 

## mutate

`mutate` lets us add or overwrite columns by computing a new value for them.

```{r}
mutate(scoretab, doublescore = score*2)
```

This is equivalent to

```{r}
scoretab2 <- scoretab
scoretab2$doublescore <- scoretab2$score * 2
scoretab2
```


## summarize

`summarize` lets us compute summaries of data.

```{r}
summarize(scoretab, total=sum(score))
```

We really want to summarize the data grouped by file, so we can see if there are any particularly bad files. This is achieved using the `group_by` "adjective". `group_by` gives the data frame a grouping using one or more columns, which modifies the subsequent call to `summarize`.

```{r}
group_by(scoretab, file)

summarize(group_by(scoretab, file), total=sum(score))
```

The special function `n()` can be used within `summarize` to get the number of rows. (This also works in `mutate`, but is most useful in `summarize`.)

```{r}
summarize(group_by(scoretab, grade), count=n())
```

So `summarize` lets us do the things we did with `table` and `tapply`, but staying tidy by using data frames.

<b>Tip:</b> `group_by` also affects other verbs, such as mutate. This is more advanced than we will be going in to today. Grouping can be removed with `ungroup`.


# The pipe %>%

We often want to string together a series of `dplyr` functions. This is achieved using `dplyr`'s new pipe operator, `%>%`. This takes the value on the left, and passes it as the first argument to the function call on the right.

```{r}
scoretab %>% group_by(grade) %>% summarize(count=n())
```

`%>%` isn't limited to `dplyr` functions. It's an alternative way of writing any R code.

```{r}
rep("hello", 5)
"hello" %>% rep(5)
```


# ggplot2


## Publication quality images

```{r}
plot <- ggplot(bigtab,aes(x=file,y=test,fill=grade)) + 
    geom_tile(color="black",size=0.5) +
    labs(x="",y="",fill="") +                 # Remove axis labels
    coord_fixed() +                           # Square tiles
    theme_minimal() +                         # Minimal theme, no grey background
    theme(panel.grid=element_blank(),         # No underlying grid lines
          axis.text.x=element_text(           # Vertical text on x axis
              angle=90,vjust=0.5,hjust=1))              
plot
```

We would ideally also reorder the x and y axes meaningfully, and refine the x axis labels.

Plots can be saved with ``ggsave``. Width and height are given in inches, and an image resolution in Dots Per Inch should also be given. The width and height will determine the relative size of text and graphics, so increasing the resolution is best done by adjusting the DPI. Compare the files produced by:

```{r, eval=FALSE}
ggsave("plot1.png", plot, width=5,  height=5,  dpi=600)
ggsave("plot2.png", plot, width=10, height=10, dpi=300)
```

It may be necessary to edit a plot further in a program such as Inkscape or Adobe Illustrator. To allow this, the image needs to be saved in a "vector" format such as SVG, EPS, or PDF. In this case, the DPI argument isn't needed.



# A small RNA-Seq example

We will now look at a small gene expression data set. To simplify somewhat: The data was generated from a multiplex PCR targetting 44 genes. The PCR product is quantified by counting reads from high throughput sequencing. The experiment is of yeast released synchronously into cell cycling, so that we can see which genes are active at different points in the cycle. Several strains of yeast have been used, a wildtype and several gene knock-downs. Each has nine time points, covering two cell cycles (two hours). 

This is much smaller than a typical RNA-Seq dataset. We are only looking at 44 genes, rather than all of the thousands of genes in the yeast genome.

Recall the three activities involved in data analysis:

<img src="../figures/data-cycle.svg" height="400">

This data set requires all of them.

## Tidying

<b>Hint:</b> You can select from the top of a pipeline to partway through and press Ctrl-Enter or Command-Enter to see the value part-way through the pipeline.

```{r}
# Use readr's read_csv function to load the file
counts_untidy <- read_csv("r-more-files/read-counts.csv")

counts <- counts_untidy %>%
    gather(sample, count, -Feature, factor_key=TRUE) %>%
    separate(sample, sep=":", into=c("strain","time"), convert=TRUE, remove=FALSE) %>%
    mutate(
        strain = factor(strain, levels=c("WT","SET1","RRP6","SET1-RRP6")),
        set1 = (strain == "SET1" | strain == "SET1-RRP6"),
        rrp6 = (strain == "RRP6" | strain == "SET1-RRP6")
    ) %>%
    filter(time >= 2) %>%
    select(sample, strain, set1, rrp6, time, gene=Feature, count)

summary(counts)
```

Here we've used two tidying functions from `tidyr`:

* `gather` gathers columns into a key column and a value column. It's similar to `reshape2`'s `melt` function.

* `separate` splits character columns by a separator string (actually, a regular expression), creating new columns.

"Time point 1" was omitted because it isn't actually part of the time series, it's from cells in an unsynchronized state. If we wanted to be even tidier, the remaining time points could be recoded with the actual time within the experiment -- they're at 15 minute intervals.


## Transformation and normalization

```{r}
ggplot(counts, aes(x=sample, y=count)) + geom_boxplot() + coord_flip()
```

We immediately see from a [Tukey](https://www.amazon.com/Exploratory-Data-Analysis-John-Tukey/dp/0201076160/ref=sr_1_1?s=books&ie=UTF8&qid=1469077563&sr=1-1) box plot that the data is far from normal -- there are outliers many box-widths to the right of the box. Perhaps a log transformation is in order.

```{r}
ggplot(counts, aes(x=sample, y=log2(count))) + geom_boxplot() + coord_flip()
```

Log transformation has greatly improved things, although now there are more outliers left of the whiskers, and some zeros we will need to be careful of. We also see one of the samples has less reads than the others.

The gene SRP68 was included as a control housekeeping gene. Its expression level should be the same in all samples. We will divide counts in each sample by the count for SRP68 to correct for library size, then log transform the data. We add a small constant when log transforming so that zeros do not become negative infinity.

```{r}
normalizer <- counts %>%
    filter(gene == "SRP68") %>%
    select(sample, norm=count)

moderation <- 0.5/mean(normalizer$norm)

counts_norm <- counts %>%
    left_join(normalizer, by="sample") %>%
    mutate(
        norm_count = count / norm, 
        log_norm_count=log2(norm_count+moderation)
    )

ggplot(counts_norm, aes(x=sample, y=log_norm_count)) + geom_boxplot() + coord_flip()
```


## Visualization

```{r}
ggplot(counts_norm,aes(x=time,y=gene,fill=log_norm_count)) + geom_tile() + facet_grid(~ strain) + scale_fill_viridis() + theme_minimal()
```

```{r}
ggplot(counts_norm,aes(x=time,y=strain,fill=log_norm_count)) + geom_tile() + facet_wrap(~ gene) + scale_fill_viridis() + theme_minimal()
```

```{r fig.width=10, fig.height=10}
ggplot(counts_norm,aes(x=time,y=log_norm_count,color=strain,group=strain)) + geom_line() + facet_wrap(~ gene, scale="free")
```

## Challenge {.challenge}

1. Make a line plot as above but just showing the gene HHF1.

2. Our normalizing gene, SRP68, isn't conveying any useful information. Remove it from the data, and redo one of the plots above.

3. (Hard) Different genes have different average expression levels, but what we are interested in is how they change over time. Further normalize the data by subtracting the average for each gene from `log_norm_count`.


# Linear modelling

*You are not expected to understand this section.* The idea is to give a taste of what is possible, so that you know to ask an expert for help if this is the sort of analysis you need. It will also provide a little context for advanced forms of RNA-Seq differential expression analysis.

```{r}
sut476 <- counts_norm %>% filter(gene=="SUT476") 

model <- lm(log_norm_count ~ strain * time, data=sut476)
model

# Note: The p-values listed in this summary aren't all meaningful.
# It makes no sense to remove a main effect term if an interaction is still present.
# Significance testing can better be done with anova() and the glht package.
summary(model)

# Columns are the actual terms in the linear model
model.matrix(log_norm_count ~ strain * time, data=sut476)

# augment() from the broom package gives various diagnostic statistics
#  .fitted   = model prediction
#  .residual = actual value minus .fitted
#  .hat      = The leverage -- how important the observation is in the design of the experiment.
#              This can be considered before performing the experiment!
#  .cooksd   = How much actual influence the observation had on the model.
#              Combines information from .residual and .hat.
augment(model, sut476) %>% head

# augment() can also be used to produce predictions for new inputs
augment(model, newdata=sut476[1:5,])

# Let's look at the model fit, and how influential each observation was
augment(model, sut476) %>%
ggplot(aes(x=time,y=log_norm_count,color=strain,group=strain)) + 
    geom_point(aes(y=log_norm_count, size=.cooksd), alpha=0.5) +
    geom_line(aes(y=.fitted)) +
    scale_size(range=c(1,10))
```

We see the initial timepoint in the wildtype strain is wildly off a linear trend, if it exists. The large Cook's Distance indicates this is strongly affecting the model. Possibly early time points need to be removed, or (more likely!) a linear trend is not a good model of this data.

See also the `anova` function for comparing nested linear models, and the `glht` package for statistical testing using contrasts.


```{r}
sessionInfo()
```



